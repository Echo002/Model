{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 数据集相关的常数设置\n",
    "INPUT_NODE = 784\n",
    "\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "LAYER1_NODE = 500  \n",
    "# 隐藏层的节点数（本结构只有一个隐藏层）\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "LRARNING_RATE_BASE = 0.8\n",
    "\n",
    "LRARNING_RATE_DECAY = 0.99  \n",
    "# 学习率的衰减率\n",
    "\n",
    "REGULARIZATION_RATE = 0.0001  \n",
    "# 正则化项在损失函数中的系数\n",
    "\n",
    "TRAINNING_STEPS = 30000  \n",
    "# 训练轮数\n",
    "\n",
    "MOVING_AVERAGE_DECAY = 0.99  \n",
    "# 滑动平均衰减率\n",
    "\n",
    "MODEL_SAVE_PATH = r\"E:\\Code\\DeepLearning\\TrainDemo\\01 MNIST\\TensorFlow\\Data\\checkPoint\"\n",
    "MODEL_NAME = \"model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数，计算神经网络的前向传播结果\n",
    "# 定义了一个生成变量的函数\n",
    "def get_weight_variable(shape, regularizer):\n",
    "    weights = tf.get_variable(\n",
    "        \"weights\", shape,\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "    )\n",
    "\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses', regularizer(weights))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络前向传播的过程\n",
    "def inference(input_tensor, regularizer):\n",
    "    # 声明第一层神经网络\n",
    "    with tf.variable_scope('layer1'):\n",
    "        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n",
    "        biases = tf.get_variable(\"biases\", [LAYER1_NODE],initializer=tf.truncated_normal_initializer(stddev=0.0))\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights)) + biases\n",
    "\n",
    "    # 声明第二层神经网络\n",
    "    with tf.variable_scope('layer2'):\n",
    "        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n",
    "        biases = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.truncated_normal_initializer(stddev=0.0))\n",
    "        layer2 = tf.matmul(layer1, weights) + biases\n",
    "\n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = inference(x, regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 损失函数、学习率、滑动平均、训练过程\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.arg_max(y_, 1), logits=y)\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(LRARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE,\n",
    "                                               LRARNING_RATE_DECAY)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # 初始化TensorFlow持久化类\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for i in range(TRAINNING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x:xs, y_:ys})\n",
    "            if i % 1000 == 0:\n",
    "                print('After %d training step(s), loss on training batch is %g' % (step, loss_value))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-fdb64d651c0d>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Echo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Echo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting E:\\DataSet\\MNIST\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Echo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting E:\\DataSet\\MNIST\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Echo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting E:\\DataSet\\MNIST\\t10k-images-idx3-ubyte.gz\n",
      "Extracting E:\\DataSet\\MNIST\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Echo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-5-d92d3731af27>:12: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "After 1 training step(s), loss on training batch is 3.01492\n",
      "After 1001 training step(s), loss on training batch is 0.331485\n",
      "After 2001 training step(s), loss on training batch is 0.180675\n",
      "After 3001 training step(s), loss on training batch is 0.155665\n",
      "After 4001 training step(s), loss on training batch is 0.130199\n",
      "After 5001 training step(s), loss on training batch is 0.110965\n",
      "After 6001 training step(s), loss on training batch is 0.111496\n",
      "After 7001 training step(s), loss on training batch is 0.0989834\n",
      "After 8001 training step(s), loss on training batch is 0.086369\n",
      "After 9001 training step(s), loss on training batch is 0.0743076\n",
      "After 10001 training step(s), loss on training batch is 0.0696236\n",
      "After 11001 training step(s), loss on training batch is 0.0687941\n",
      "After 12001 training step(s), loss on training batch is 0.0685555\n",
      "After 13001 training step(s), loss on training batch is 0.0657666\n",
      "After 14001 training step(s), loss on training batch is 0.0542264\n",
      "After 15001 training step(s), loss on training batch is 0.0528909\n",
      "After 16001 training step(s), loss on training batch is 0.0491639\n",
      "After 17001 training step(s), loss on training batch is 0.0601289\n",
      "After 18001 training step(s), loss on training batch is 0.0435834\n",
      "After 19001 training step(s), loss on training batch is 0.0443124\n",
      "After 20001 training step(s), loss on training batch is 0.0467378\n",
      "After 21001 training step(s), loss on training batch is 0.0431454\n",
      "After 22001 training step(s), loss on training batch is 0.0389455\n",
      "After 23001 training step(s), loss on training batch is 0.037909\n",
      "After 24001 training step(s), loss on training batch is 0.0418152\n",
      "After 25001 training step(s), loss on training batch is 0.0442685\n",
      "After 26001 training step(s), loss on training batch is 0.0351491\n",
      "After 27001 training step(s), loss on training batch is 0.032003\n",
      "After 28001 training step(s), loss on training batch is 0.0401646\n",
      "After 29001 training step(s), loss on training batch is 0.0337195\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Echo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3273: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(argv = None):\n",
    "    mnist = input_data.read_data_sets(r\"E:\\DataSet\\MNIST\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
